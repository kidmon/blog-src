---
toc: true
widgets:
  - type: toc
    position: right
  - type: category
    position: right
  - type: recent_posts
    position: right
sidebar:
  right:
    sticky: true
title: 워드 임베딩(Word Embedding)
date: 2019-10-20 02:22:22
categories: [NLP, Word Embedding]
tags: [NLP, Word Embedding, Word2Vec]
thumbnail: /image/word-embedding/thumbnail.png
---

워드 임베딩(Word Embedding)은 최근 자연어 처리(Natural Language Processing) 분야에 있어 거의 필수적으로 사용되고 있다고 할 수 있을 만큼 아주 강력한 방법입니다. 단어를 벡터로 표현함으로써 다양한 연산을 가능하게 하고, 이것을 통해 단어 사이의 유사도를 계산할 수 있습니다.

<!-- more -->

## 임베딩(Embedding)이란 무엇인가?

워드 임베딩에 대해 알아보기 전에 임베딩이 무엇인지 알아보겠습니다. Embed의 사전적 의미는 ***끼워 넣다, (단단히)박다*** 라는 뜻입니다.

<center>
<br>

![Embed의 사전적 의미](https://paper-attachments.dropbox.com/s_451DA3DB5AFFC3A9D76904DC1472434BD18C04577226314D20B4F28ECC2606C1_1570892054223_1.PNG)

<br>
</center>

그렇다면 Embedding은 무엇을 어디에 끼워 넣는다는 뜻일까요?? 바로 `데이터를 벡터 차원으로` 끼워 넣는 것입니다. 컴퓨터의 관점으로 보자면 `Discrete 데이터를 Continuous 벡터로 변환`시키는 것이죠.

> 범주형 자료(Discrete Variable)를 연속적인 벡터(Continuous Vector)의 형태로 변환시키는 것.

자연어 처리 분야에서 임베딩은 `용어(Term)사이의 유사점을 계산`하기 위해 사용됩니다. 예로는 텍스트 분류, 문서 클러스터링, 품사 태깅, 엔티티 인식, 정서 분석 등이 있습니다.

## 왜 임베딩을 사용하는가?

앞서 범주형 자료를 연속적인 벡터로 만들 수 있다고 말씀드렸는데, 그럼 이렇게 임베딩을 함으로써 얻을 수 있는 것은 무엇일까요? 예를 한 번 들어보겠습니다. 우리는 `아메리카노와 카페라떼의 관계`를 알고 있습니다. 둘 다 커피의 한 종류로써 에스프레소와 어떤 것을 섞어서 만들었냐의 차이죠. 컴퓨터는 과연 아메리카노와 카페라떼라는 두 단어만 보았을 때 어떤 관계인지 알 수 있을까요?

잠시 기초적인 부분으로 돌아가서, 컴퓨터는 모든 것을 숫자로 받아들이고 처리합니다. 그렇다면 컴퓨터가 두 단어 사이의 관계를 알 수 있게 하려면 `단어를 숫자로 변환`해서 알려주는 것이 가장 좋은 방법이겠죠.

## 원-핫 인코딩(One-hot Encoding)

자연어 처리에는 문자를 숫자로 바꾸는 여러 가지 기법들이 있습니다. 그중 단어를 숫자로 표현하는 가장 기본적인 방법이 바로 `원-핫 인코딩(One-hot Encoding)`입니다.

원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하며, 표현하고 싶은 단어의 인덱스에 `1`의 값을 부여하고 다른 인덱스에는 `0`을 부여하는 단어의 벡터 표현 방식입니다. 이렇게 표현된 벡터를 `원-핫 벡터(One-hot Vector)`라고 합니다.

<center>
<br>

![커피의 종류가 다섯가지 뿐인 세상…ㅠㅠ](https://paper-attachments.dropbox.com/s_451DA3DB5AFFC3A9D76904DC1472434BD18C04577226314D20B4F28ECC2606C1_1570894212830_2.PNG)

<br>
</center>

커피의 종류가 다섯 가지인 세상이 있다고 가정하고, 원-핫 인코딩 사용해서 아메리카노를 벡터로 표현한다면

> **아메리카노 = (1, 0, 0, 0, 0)**

처럼 나타낼 수 있는 것이죠. 이해가 되셨나요?

## 희소 표현(Sparse Representation)

위에서 보여드린 예시처럼 대부분의 값이 0으로 표현되는 방식을 `희소 표현(Sparse Representation)`이라 하고, 희소 표현으로 나타낸 벡터를 `희소 벡터(Sparse Vector)`라고 합니다. 그러나 희소 표현에는 한 가지 문제가 있습니다. 이미 눈치채셨겠지만, 단어가 많아지면 많아질수록 벡터의 차원 또한 커지게 된다는 것입니다.

다시 한 번 예를 들면, 커피의 종류가 10,000개인 세상에서는 아메리카노를 벡터로 표현하기 위해서

> **아메리카노 = (1, 0, 0, 0, 0, 0, 0, 0, 0, 0 … *9,990개의 0* )** [10,000의 차원]

처럼 나타낼 수 있습니다.

또한, 희소 표현의 특성으로 인해 아메리카노 하나를 표현하기 위해서 10,000개의 차원을 사용한 것도 모자라 그중 9,999개는 0이라는 엄청난 공간적 낭비가 생기는 거죠.

## 밀집 표현(Dense representation)

이러한 희소 표현과 반대되는 표현을, 이를 `밀집 표현(Dense Representation)`이라 하고, 밀집 표현으로 나타낸 벡터를 `밀집 벡터(Dense Vector)`라고 합니다.

밀집 표현은 벡터의 차원을 단어 집합의 크기로 가지지 않습니다. 10,000개의 단어를 표현하는데 100개의 차원을 설정한다면 10,000개의 단어의 벡터 표현을 100개의 차원으로 변환합니다. 또한, 벡터는 더 이상 0과 1만 가지는 것이 아니라 `실수`를 벡터의 값으로 가지게 됩니다.

> **아메리카노 = (0.1, 2.4, 1.1, -2.1, 0.7, … *95개의 실수*)** [100의 차원]

처럼 나타낼 수 있는 것입니다. 그럼 과연 어떤 방법으로 희소 벡터를 밀집 벡터로 나타낼 수 있을까요?

## 워드 임베딩(Word Embedding)

희소 벡터를 밀집 벡터로 변환하는 것을 가능하게 하는 방법이 바로 `워드 임베딩(Word Embedding)`입니다. `인공신경망(Artificial Neural Network)`을 이용하여 원-핫 인코딩으로 만들어진 희소 벡터를 학습시켜 밀집 벡터로 변환시키는 것이죠.

인공신경망을 간단하게 소개드리면, 생물학의 신경망에서 영감을 얻은 통계학적 학습 알고리즘으로 입력계층(Input Layer), 은닉계층(Hidden Layer), 출력계층(Output)으로 이루어진 모델입니다.

<center>
<br>

![인공신경망 모델](https://paper-attachments.dropbox.com/s_451DA3DB5AFFC3A9D76904DC1472434BD18C04577226314D20B4F28ECC2606C1_1570901653956_3.png)

<br>
</center>

워드 임베딩은 `문맥 정보`만이 언어 항목의 실행 가능한 표현을 구성한다는 아이디어에 기반을 두고 있습니다. 즉, 언어가 맥락적(주변 요소들과의 관계를 통해 특성을 가짐)이라는 가정을 하고 있다는 것입니다.

## Word2Vec

워드임베딩을 구현하는 방법 중 가장 널리 알려지고 많이 사용되는 것이 바로 `Word2Vec`입니다. Word2Vec이 실제로 어떻게 작동하는지 보고 싶은 분들은 [여기](http://word2vec.kr/search/)에 들어가서 확인하실 수 있습니다. Word2Vec을 한글에 적용하여 확인해볼 수 있도록 구현한 사이트입니다.

<center>
<br>

![벡터로 표현된 단어 간의 연산](https://paper-attachments.dropbox.com/s_451DA3DB5AFFC3A9D76904DC1472434BD18C04577226314D20B4F28ECC2606C1_1570905454652_4.PNG)

<br>
</center>

위의 예제를 보면 알수있다시피, `한국+서울-도쿄`처럼 단어 간의 연산(덧셈과 뺄셈)을 할 수 있도록 되어있습니다. `단어가 벡터로 표현`되었기 때문이죠.

Word2Vec에서 학습 방법으로 사용된 모델은

1. ***CBOW(Continuous Bag of Words)***
2. ***Skip-Gram***

두 가지입니다. CBOW는 `주변 단어(맥락)를 가지고 단어를 예측`하고, 반대로 Skip-Gram은 `단어를 이용해서 주변 단어(맥락)를 예측`하는 모델입니다.

## CBOW

이번에는 CBOW 모델의 작동 원리를 간단하게 알아보겠습니다. CBOW는 주변 단어(맥락)로 타겟 단어를 예측하는 모델입니다. 주변 단어란 보통 타겟 단어의 앞, 뒤에 있는 몇 단어를 뜻합니다. 이 주변 단어의 범위는 `윈도우(Window)`라고 부릅니다.

예를 들어

> 바닐라라떼는 우유에 에스프레소를 넣고 바닐라 시럽을 뿌려준 커피입니다.

라는 문장이 있다고 해봅시다.

간단하게 전처리를 하면 이런 형태가 되겠죠.

> 바닐라라떼 우유 에스프레소 넣다 바닐라 시럽 뿌리다 커피 이다

주변 단어의 범위를 타겟 단어의 앞, 뒤로 한 단어라고 정의하고 윈도우를 만들어 보겠습니다. 아래의 그림과 같이 타겟 단어를 `우유`라고 한다면 주변 단어는 `바닐라라떼`와 `에스프레소`가 되는 것입니다.

또한 `슬라이딩 윈도우(Sliding Window)`라는 방식으로 타겟 단어와 주변 단어를 바꿔가면서 학습을 위한 데이터 셋을 만듭니다.

<center>
<br>

![Sliding Window](https://paper-attachments.dropbox.com/s_451DA3DB5AFFC3A9D76904DC1472434BD18C04577226314D20B4F28ECC2606C1_1571812124151_CBOW.png)

<br>
</center>

CBOW는 주변 단어로 타겟 단어를 예측하는 모델이라고 말씀드렸습니다. 그럼 모델의 입력이 주변 단어, 출력이 타겟 단어가 되겠죠. 입력과 출력은 벡터화되어서 벡터 공간에서 표현됩니다. 즉, `주변 단어가 비슷하면 타겟 단어의 벡터 표현 역시 비슷해지는 것`입니다.

벡터가 비슷하다는 말은 벡터 공간상에서 가까이 위치한다(거리가 가깝다)는 의미입니다. Word2Vec은 이러한 방식으로 비슷한 맥락의 단어에 비슷한 벡터를 줍니다.

<center>
<br>

![CBOW](https://files.slack.com/files-pri/T25783BPY-F6CPDPKHP/screenshot.png?pub_secret=8ce6c11e3b)

<br>
</center>

이번에는 CBOW의 뉴럴 네트워크 모델을 보겠습니다. 위의 모델에서 입력은 One-hot Encoding된 벡터입니다. 입력, 즉 주변 단어는 *V*개의 요소 중 하나만 1이고 나머지는 모두 0인 벡터로 표현됩니다. 이렇게 단어의 개수만큼의 차원을 갖는 입력 레이어(Input Layer)가 히든 레이어(Hidden Layer)에서 임베딩 크기 *N*만큼의 차원의 벡터로 대응됩니다. 마지막으로 출력 레이어(Output Layer)는 다시 단어의 개수만큼의 차원을 가지는 벡터로 변환됩니다. 출력은 타겟 단어이므로 단어의 개수만큼의 경우의 수가 있기 때문입니다.

<!--
레이어들 사이의 뉴런들은 서로 모두 연결되어 있습니다. 입력 레이어와 히든 레이어 사이를 연결하는 파라미터들은 행렬 $W_{V \times N}$로 나타낼 수 있고, 입력 레이어에서 히든 레이어로 넘어가는 것은 단순히 행렬 $W$를 곱하는 것과 같습니다. $x$가 입력 벡터라고 하면, 히든 레이어 $h$로 임베딩된 벡터는 $W^{T}x$ 로 나타낼 수 있습니다.
입력 벡터 $x$는 one-hot encoding된 벡터입니다. $x$의 요소 중 $k$번째 요소만 1이라고 하겠습니다. $x$의 나머지 요소가 모두 0이기 때문에 다른 부분은 모두 무시되고 $W^{T}x$ 의 결과는 $W^{T}$ 의 $k$번째 열, 즉 $W$의 $k$번째 행만 남게됩니다 이 벡터가 해당 단어의 $N$차원 벡터 표현이 되는 것입니다. $W$의 각 행들은 각각 해당하는 단어의 $N$차원의 벡터 표현인 거죠. $W$의 $i$번째 행을 $v^{T}_w_{i}$라고 부르면, 히든 레이어 h는 $v^{T}_{w_{i}}$와 결국 같다는 것을 알 수 있습니다.
$h=W^{T}x=W^{T}_{k_{i}}:=v^{T}_{w_{i}}$
입력 레이어에서 히든 레이어로 넘어가면서 우리는 히든 레이어 $h$를 얻었습니다. 히든 레이어에서 출력 레이어로 넘어가기 위해, 우리는 또다른 행렬$W'$가 필요합니다.  $W'$는 $N \times V$의 행렬입니다. 이 파라미터 행렬을 이용해서, 우리는 모든 단어에 대해 출력 레이어의 점수 $u_{j}$를 계산할 수 있습니다. 아래 식에서$v'_{w_{j}}$는 $W'$의 $j$번쩨 열을 뜻합니다. 즉 $u_{j}$ 는 $j$번째 단어에 대한 예측 점수라고 볼 수 있습니다.
$u_{j}={v'}^{T}_{w_{j}}$
마지막으로 예측 점수를 각 단어의 확률값으로 바꿔주기 위해 softmax를 사용합니다. 이는 각 단어의 점수에 비례하여 점수를 확률로 만들어주는 방법입니다. 이 방식을 통해 각 단어의 예측 점수가 모두 0 이상이고 모두 더하면 1이 되는 확률값으로 변합니다.
$p(w_{j}|w_{i})=y_{j}=\frac{exp(u_{j})}{\Sigma^{V}_{j'}exp(u_{j'})}$
$\frac{a}{exp(u_{j})}$
여기에서 $y_{j}$는 출력 레이어의 $j$번째 출력값입니다. 위 식들을 조합하면 최종적으로 아래와 같은 식을 얻을 수 있습니다.
결과적으로 단어 $w$는 $v_{w}$ 와 $v'_{w}$ 이라는 벡터로 표현됩니다.$v_{w}$는 입력 레이어에서 히든 레이어로 넘어가는 행렬 $W$에서 나오며, $v'_{w}$는 히든 레이어에서 출력 레이어로 넘어가는 행렬 $W'$ 에서 나옵니다. $v_{w}$를 단어 $w$의 입력 벡터(Input Vector), $v'_{}$를 단어 $w$의 출력 벡터(Output Vector)라고도 부릅니다.
-->

## 마치며
이번 포스팅에서는 워드 임베딩에 대한 기초와 가장 대표적인 모델인 Word2Vec에 대해서 간단히 알아보았습니다. 자연어를 처리하는 방법에는 임베딩 말고도 규칙 기반 모델이나 확률 모델 등 여러 가지가 있겠습니다만, 최근 트렌드인 인공지능과 기계학습 등을 활용한 연구를 위해서는 거의 필수적이라고 할 수 있을 만큼 중요한 방법이 되었다고 생각합니다.

부족한 글이지만 저와 마찬가지로 자연어 처리 공부를 처음 시작하고자 하는 분들에게 조금이나마 도움이 될 수 있었으면 좋겠습니다. 끝으로 포스팅을 작성하는데 참고할 수 있도록 좋은 내용을 잘 정리해주신 분들께 감사의 인사를 올리며 링크 출처를 남깁니다.

---

## 참고한 글
1. [쉽게 씌어진 word2vec](https://dreamgonfly.github.io/machine/learning,/natural/language/processing/2017/08/16/word2vec_explained.html)
2. [워드 임베딩의 간략한 역사](https://brunch.co.kr/@geumjaelee/5)
3. [딥러닝을 이용한 자연어 처리 입문](https://wikidocs.net/22644)
